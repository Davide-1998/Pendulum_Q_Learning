\documentclass[twocolumn, a4paper]{article}
\usepackage[top=1cm, bottom=1.2cm, left=1.5cm, right=1.5cm]{geometry}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{float}
\usepackage{amsmath}
\usepackage{hyperref}

% \setlength\intextsep{0pt}
% \setlength{\columnsep}{0.6cm}

\begin{document}
\title{
	   \LARGE\textbf{Third Assignment ORC\\DQN Network}
	   \vspace{1cm}
	  }
\author{
		\textbf{Bertelli Davide} \\
		mat. 223718 \\
		davide.bertelli-1@studenti.unitn.it
		\and
		\textbf{Hueller Jhonny} \\
		mat. 221254\\
		jhonny.hueller@studenti.unitn.it
	   }
\date{}
\maketitle

\section{Abstract}
The third assignment revolved around the task to implement a deep Q-Learning
algorithm able to proficiently tackle the task to balance a pendulum.
This work focuses on the adaptation of the work in \cite{Mnih} to a simpler
environment, along with many suggestion from \cite{Roderick} for
implementing the details.
The resulting deep neural network is able to accurately estimate the Q-function
of the pendulum along with a greedy policy which together allows it to produce 
episodes with low cost, managing to balance the pendulum starting from any state.

\section{Introduction}
The pendulum is simulated having a continuous state space, which represents
the position and velocity of its joint variables, and a discrete control
space, which represents the joint torques requested by the controller.
Given this circumstances it is not possible to execute traditional Q-learning
and produce the Q-table necessary for estimating the Q-function, being the table
incompatible with continuous state spaces.
This is one of the reasons it might be preferable, if not required, to use
V/Q-function approximation techniques.
As instructed by \cite{Roderick}, In this work the Q-function has been
approximated using a deep neural network, which forced to implement several
new components into the training in order to ensure a good convergence.

\section{Components of Deep Q-Network}
The deep Q-learning algorithms introduce 3 majaor components: a deep neural
network used to approximate the Q function, the use of mini-batches and
experience replay in order to aid the proper execution of stochastic gradient
descent, and the use of old network parameters in order to estimate the Q
target for the loss function\cite{Roderick}. Another important component,
one which is not exclusive to deep Q-network, is the policy used for learning
the Q-function.

\section{Network}
The network in use is composed of several fully connected layers which receive
as input the state of the pendulum, described as the position in
radiants and the angular velocity of each joint.
The last output layer produces the predicted Q value for each possible control
of the joints in the specific state.
Specifically the network is composed of: input layer, 4 dense hidden layers
having respectively 16, 32, 64 and 64 activation cells, and the output layer
having dimension equal to the number of discrete controls.
The number of controls is \(|U|=resolution^{\#joints}\), which
means that if \(resolution=15\) and \(\#joints=2\), \(|U|=15^{2}=225\).
In Figure \ref{fig:Network} is shown a graphical representation of the network
in use.

\label{fig:Network}
\begin{figure}
	\centering
	\includegraphics[width=8.5cm]{"../Figures/Network_schema"}
	\caption{Network architecture in case of $n\_joints$ composing the pendulum
			 and a control discretization over 15 steps.}
\end{figure}

\section{System setup}
The system provided as solution is composed by several objects:
\begin{itemize}
	\item \textbf{Loss Function}:The training loss uses the common least
		  squares method between the target \(Q^{-}\) values, produced using
		  the current cost and the discounted minimum estimated cost of the
		  next state using old network parameters, and the current \(Q\) value,
		  produced by the up-to-date network parameters:
		  \(L(Q^{-},Q)=\sum_{i}(c_{i}+\gamma\min_{u'}Q^{-}(x_{i+1},u')-Q(x_{i},
		  u_{i}))^{2}\). The necessary data for the loss function, like state
		  and control, are obtained using experience replay.
	\item \textbf{Experience Replay}: Experience replay is a technique used in
		  order to remove the correlation between sequences of data. Being
		  \((x_{i}, u_{i}, c_{i}, x_{i+1})\) respectively the state, control,
		  cost, and next state of the system at time \(i\), the experience
		  replay buffer is composed of the list of transitions
		  \({t_{i}=(x_{i}, u_{i}, c_{i}, x_{i+1}) \forall i\in [n, n+s)}\)
		  the system produced during training and from which we randomly
		  sample a mini-batch to be used for stochastic gradiant descent.
		  Since the experience replay buffer has a limited size
		  \(s\leq maximum\:size\) old transitions are progressively removed as
		  new transitions are added when the size limit is reached.
	\item \textbf{Policy}: The policy used during training is an
		  \(\epsilon-greedy\) policy, similar to the one used in \cite{Mnih},
		  meaning a policy which select a random action with probability
		  \(\epsilon\) and chooses the optimal action with probability
		  \(1-\epsilon\). This behaviour is desirable due to the necessity of
		  exploration during training. In a similar way to \cite{Mnih} we have
		  used an exponentially decreasing \(\epsilon\), from \(max=1\) to
		  \(min=0.01\), following the equation:
		  \(\epsilon=max(min,e^{\frac{ln(min)}{0.75N}\times n})\), with
		  \(n=\text{Episode index}\) and \(N=\text{Number of episodes}\).
		  The decaying has been conceived in order to adapt to the number of
		  training episodes and reach the minimum epsilon value after executing
		  75\% of the episodes.
\end{itemize}

\label{fig:TrainLoss1}
\begin{figure}[H]
	\centering
	\includegraphics[width=8.5cm]{"../Figures/epsilon_1J_500E_256EL_11RES.png"} \\
	\caption{Epsilon graph over 500 training episodes.}
	\includegraphics[width=8.5cm]{"../Figures/epsilon_1J_1000E_256EL_11RES.png"}
	\caption{Epsilon graph over 1000 training episodes.}
\end{figure}

\subsection{Actor Network Training}
To run the training the user is required to provide two mandatory values: the
number of episodes and their length, defined as number of steps.
These two parameters define, respectively, the number of the random starting
positions from which the robot will start the training and the length of the
search executed for each episode. At each step, the current state of the
pendulum, \(x_{i}\), is processed by the policy, which produce the control
\(u_{i}\) for this step. The control is applied on the pendulum, producing the
next state \(x_{i+1}\) and the cost \(c_{i}\) of the pair
\((x_{i},u_{i})\). This data are integrated as the transition \(t_{i}=(x_{i},
u_{i}, c_{i}, x_{i+1})\) and added into the experience replay buffer. Once
every \(k_{1}\) steps we proceed to perform a gradient descent execution,
updating the current \(Q\)-network's parameters by sampling a mini-batch of
random transitions from the experience replay buffer and back-propagating the
loss described previously using Adam as optimizer. After gradient descent once
every \(k_{2}\) steps we update the \(Q\)-target network's parameters to match
the current \(Q\) network ones.

\section{Actor Network Testing}
The testing of the actor network was carried out first, varying the starting
position of the robot randomly and secondly making it start from the lowest
position to ensure the goodness of the results.
For each configuration three episodes were run and the sum of costs gathered
during their total length is used as the evaluation measure.
The testing phase works by gathering the robot state, querying the policy
for the next action, apply it and take the cost from the environment for all
the length of the episode.

\section{Experiments}
% 1 joint, 2 joints, N-joints
% Training time
% Changing resolution of the controls
The system has been tested over different configurations of the robot,
specifically with 1 and 2 joints, different control resolution and
different number and length of episodes.
The evaluation metrics used are the loss of the training, the training time
and the total cost in the testing phase.

\section{Results}
\subsection{1 joint}
Following are shown the results in case the robot has 1 joint.

\label{fig:TrainLoss_1_500_11}
\begin{figure}[H]
	\centering
	\includegraphics[width=8.5cm]{"../Figures/training_loss_1J_500E_256EL_11RES.png"}
	\caption{Training loss over 500 episodes lasted 256 iterations having 1
			 joint and 11 resolution steps.}
\end{figure}
\vspace{-1cm}

\label{fig:TrainLoss_1_500_17}
\begin{figure}[H]
	\centering
	\includegraphics[width=8.5cm]{"../Figures/training_loss_1J_500E_256EL_17RES.png"}
	\caption{Training loss over 500 episodes lasted 256 iterations having 1
			 joint and 17 resolution steps.}
\end{figure}
\vspace{-1cm}

\label{fig:TrainLoss_1_1000_11}
\begin{figure}[H]
	\centering
	\includegraphics[width=8.5cm]{"../Figures/training_loss_1J_500E_256EL_11RES.png"}
	\caption{Training loss over 1000 episodes lasted 256 iterations having 1
			 joint and 11 resolution steps.}
\end{figure}
\vspace{-1cm}

\label{fig:TrainLoss_1_1000_17}
\begin{figure}[H]
	\centering
	\includegraphics[width=8.5cm]{"../Figures/training_loss_1J_500E_256EL_11RES.png"}
	\caption{Training loss over 1000 episodes lasted 256 iterations having 1
			 joint and 17 resolution steps.}
\end{figure}
\vspace{-1cm}

% \label{fig:TrainTime1}
% \begin{figure}[H]
% 	\centering
% 	\includegraphics[width=8.5cm]{"../Figures/training_time_over_epiodes_1J_500E_256EL.png"}
%	\caption{Training time having 1 joints.}
% \end{figure}

\label{fig:Test_1_random_pos}
\begin{figure}[H]
	\centering
	\includegraphics[width=8.5cm]{"../Figures/loss_last_ep_random_positions_1J_500E_256EL.png"}
	\caption{Total cost over the test episodes for the robot starting
			 from random positions.}
\end{figure}
\vspace{-1cm}
\label{fig:Test_1_down_pos}
\begin{figure}[H]
	\centering
	\includegraphics[width=8.5cm]{"../Figures/loss_last_ep_down_positions_1J_500E_256EL.png"}
	\caption{Total cost over the test episodes for the robot starting
			 from the down position.}
\end{figure}
\vspace{-1cm}
\label{fig:Test_1_best_random_pos}
\begin{figure}[H]
	\centering
	\includegraphics[width=8.5cm]{"../Figures/loss_best_net_random_positions_1J_500E_256EL.png"}
	\caption{Total cost over the test episodes for the robot starting from
			 random positions using the best performing network.}
\end{figure}
\vspace{-1cm}
\label{fig:Test_1_best_down_pos}
\begin{figure}[H]
	\centering
	\includegraphics[width=8.5cm]{"../Figures/loss_best_net_down_positions_1J_500E_256EL.png"}
	\caption{Total cost over the test episodes for the robot starting from
			 down positions using the best performing network.}
\end{figure}

\subsection{2 joints}
Following are shown the results in case the robot has 2 joints.


\subsection{General remarks}
% As shown in figures \ref{fig:TrainLoss1}, \ref{fig:TrainLoss2}
we can observe how the loss during
training shows a progressive reduction with the increasing number of episodes.
The spikes in the graph has to be attributed to the exploration task that
is conducted by the policy.

In the testing phase it is possible to observe that the cumulative cost
saturates meaning that the robot is performing actions leading to the best
solution possible which is returning a cost almost 0.

Regarding the training time plots, it is possible to observe how independently
by the number of joints the time required for the computations saturates
without exceeding the 0.8 seconds per episode.
\newpage
\section{Conclusions}

\begin{thebibliography}{9}
	\bibitem{Mnih}
	Mnih, V., Kavukcuoglu, K., Silver, D. et al. Human-level control through
	deep reinforcement learning. Nature 518, 529–533 (2015).
	https://doi.org/10.1038/nature14236
	\bibitem{Roderick}
	Roderick M., MacGlashan J., Tellex S. Implementing the Deep Q-Network.
	\href{https://doi.org/10.48550/arXiv.1711.07478}
		 {https://doi.org/10.48550/arXiv.1711.07478}
\end{thebibliography}

\end{document}
